{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d760897",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from natsort import natsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a3e1834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# User configuration - adjust these paths as needed\n",
    "# -----------------------------\n",
    "SOURCE_DIR = Path(\"collect_data\")  # Path to folder containing .pt files and sensor/top_cam\n",
    "OUTPUT_DIR = Path(\"training_data\")    # Path where GR00T-formatted dataset will be created\n",
    " \n",
    "# -----------------------------\n",
    "# Step 1: Create target directory structure\n",
    "# -----------------------------\n",
    "# data/chunk-000 for .parquet files\n",
    "(OUTPUT_DIR / \"data\" / \"chunk-000\").mkdir(parents=True, exist_ok=True)\n",
    "# videos/chunk-000/observation.images.ego_view for image frames\n",
    "(OUTPUT_DIR / \"videos\" / \"chunk-000\" / \"observation.images.ego_view\").mkdir(parents=True, exist_ok=True)\n",
    "# meta directory\n",
    "(OUTPUT_DIR / \"meta\").mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ae7c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Step 2: Load all .pt tensors\n",
    "# -----------------------------\n",
    "def load_tensor(path: Path):\n",
    "    \"\"\"Helper to load a torch tensor from a .pt file.\"\"\"\n",
    "    return torch.load(path, map_location=\"cpu\",weights_only=True)\n",
    "\n",
    "\n",
    "global_idx = 0\n",
    "\n",
    "def parquet_file_generation(idx, folder, global_idx):\n",
    "    gripper_state = load_tensor(folder / \"Panda 10 follower\" / \"gripper_state.pt\")      # shape: [686]\n",
    "    # print(gripper_state.shape)\n",
    "    joint_pos = load_tensor(folder / \"Panda 10 follower\" / \"joint_pos.pt\")       # shape: [686, 7]\n",
    "    # print(joint_pos.shape)\n",
    "    # joint_vel = load_tensor(folder / \"Panda 10 follower\" / \"joint_vel.pt\")       # shape: [686, 7]\n",
    "    # print(joint_vel.shape)\n",
    "    \n",
    "    # timestamps tensor\n",
    "    timestamps = load_tensor(folder / \"timestamps.pt\")                                  # shape: [686]\n",
    "\n",
    "    task_file = (folder/\"task\")\n",
    "    \n",
    "    with open(task_file, \"r\") as f:\n",
    "        task_idx = int(f.read().strip())\n",
    "    print(f\"task_idx: {task_idx}\")\n",
    "\n",
    "    num_steps = gripper_state.shape[0] - 1\n",
    "\n",
    "    # Step 3: Write each time step as a single-row Parquet file\n",
    "    # -----------------------------\n",
    "    state_parts = OrderedDict([\n",
    "        (\"joint_pos\",    joint_pos[:-1,:]),\n",
    "        (\"gripper_state\", gripper_state[:-1]),\n",
    "        # (\"joint_vel\",    joint_vel[:-1,:]),\n",
    "    ])\n",
    "    action_parts = OrderedDict([\n",
    "        (\"joint_pos\",    joint_pos[1:,:]),\n",
    "        (\"gripper_state\", gripper_state[1:]),\n",
    "        # (\"joint_vel\",    joint_vel[1:,:]),\n",
    "    ])\n",
    "\n",
    "    rows = []\n",
    "    for i in range(num_steps): \n",
    "        state_vec = np.concatenate([\n",
    "            arr[i].cpu().numpy().ravel() for arr in state_parts.values()\n",
    "        ])\n",
    "        action_vec = np.concatenate([\n",
    "            arr[i].cpu().numpy().ravel() for arr in action_parts.values()\n",
    "        ])\n",
    "        row = {\n",
    "            \"observation.state\": state_vec.tolist(),\n",
    "            \"action\":            action_vec.tolist(),\n",
    "            \"timestamp\":         float(timestamps[i].item()),\n",
    "            \"annotation.human.action.task_description\": task_idx, # index of the task description in the meta/tasks.jsonl file\n",
    "            \"task_index\":        task_idx, # index of the task in the meta/tasks.jsonl file\n",
    "            # \"annotation.human.validity\": 1, \n",
    "            \"episode_index\":     idx,\n",
    "            \"index\":             global_idx,\n",
    "            \"next.reward\":       0.0,\n",
    "            \"next.done\":         False,\n",
    "        }\n",
    "        rows.append(row)\n",
    "        global_idx += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    df_path = OUTPUT_DIR / \"data\" / \"chunk-000\" / f\"episode_{idx:06d}.parquet\"\n",
    "    df.to_parquet(df_path)\n",
    "    print(f\"episode_{idx:06d}.parquet is generated: {df_path}\")\n",
    "\n",
    "    episodes_jsonl_data = {\"episode_index\":idx,\"tasks\":task_idx,\"length\":num_steps}\n",
    "    with open(OUTPUT_DIR / \"meta\"/ \"episodes.jsonl\", \"a\") as f:\n",
    "        f.write(json.dumps(episodes_jsonl_data) + \"\\n\")\n",
    "\n",
    "    return global_idx\n",
    "\n",
    "\n",
    "def video_generation(idx: int, folder: Path):\n",
    "\n",
    "    src = folder / \"sensors\" / \"top_cam\"\n",
    "    output = OUTPUT_DIR / \"videos\" / \"chunk-000\" / \"observation.images.ego_view\" / f\"episode_{idx:06d}.mp4\"\n",
    "    output.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    image_files = natsorted(src.glob(\"*.*\"))\n",
    "    frames = []\n",
    "    for img_path in image_files:\n",
    "        arr = imageio.imread(str(img_path))\n",
    "        img = Image.fromarray(arr).resize((256, 256), Image.LANCZOS)\n",
    "        frames.append(np.array(img))\n",
    "\n",
    "    # ffmpeg_params = [\n",
    "    #     \"-pix_fmt\", \"yuv420p\",        \n",
    "    #     \"-preset\", \"fast\",            \n",
    "    #     \"-crf\", \"23\",                \n",
    "    #     \"-vsync\", \"passthrough\",      \n",
    "    # ]\n",
    "\n",
    "    with imageio.get_writer(\n",
    "        str(output),\n",
    "        format=\"FFMPEG\",\n",
    "        mode=\"I\",\n",
    "        fps=20,\n",
    "        codec=\"libx264\",\n",
    "        # ffmpeg_params=ffmpeg_params\n",
    "    ) as writer:\n",
    "        for frame in frames:\n",
    "            writer.append_data(frame)\n",
    "\n",
    "    print(f\"video episode_{idx:06d}.mp4 is generated: {output}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b67b27bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************************************************************************\n",
      "0: collect_data/2025_06_20-15_04_59\n",
      "task_idx: 0\n",
      "episode_000000.parquet is generated: training_data/data/chunk-000/episode_000000.parquet\n",
      "****************************************************************************************************\n",
      "1: collect_data/2025_06_20-15_24_22\n",
      "task_idx: 0\n",
      "episode_000001.parquet is generated: training_data/data/chunk-000/episode_000001.parquet\n",
      "****************************************************************************************************\n",
      "2: collect_data/2025_06_20-15_26_37\n",
      "task_idx: 0\n",
      "episode_000002.parquet is generated: training_data/data/chunk-000/episode_000002.parquet\n",
      "****************************************************************************************************\n",
      "3: collect_data/2025_06_20-15_27_29\n",
      "task_idx: 0\n",
      "episode_000003.parquet is generated: training_data/data/chunk-000/episode_000003.parquet\n",
      "****************************************************************************************************\n",
      "4: collect_data/2025_06_20-15_28_15\n",
      "task_idx: 0\n",
      "episode_000004.parquet is generated: training_data/data/chunk-000/episode_000004.parquet\n",
      "****************************************************************************************************\n",
      "5: collect_data/2025_06_20-15_28_48\n",
      "task_idx: 0\n",
      "episode_000005.parquet is generated: training_data/data/chunk-000/episode_000005.parquet\n",
      "****************************************************************************************************\n",
      "6: collect_data/2025_06_20-15_31_08\n",
      "task_idx: 0\n",
      "episode_000006.parquet is generated: training_data/data/chunk-000/episode_000006.parquet\n",
      "****************************************************************************************************\n",
      "7: collect_data/2025_06_20-15_32_07\n",
      "task_idx: 0\n",
      "episode_000007.parquet is generated: training_data/data/chunk-000/episode_000007.parquet\n",
      "****************************************************************************************************\n",
      "8: collect_data/2025_06_20-15_33_33\n",
      "task_idx: 0\n",
      "episode_000008.parquet is generated: training_data/data/chunk-000/episode_000008.parquet\n",
      "****************************************************************************************************\n",
      "9: collect_data/2025_06_20-15_34_13\n",
      "task_idx: 0\n",
      "episode_000009.parquet is generated: training_data/data/chunk-000/episode_000009.parquet\n",
      "****************************************************************************************************\n",
      "10: collect_data/2025_06_20-15_35_13\n",
      "task_idx: 0\n",
      "episode_000010.parquet is generated: training_data/data/chunk-000/episode_000010.parquet\n",
      "****************************************************************************************************\n",
      "11: collect_data/2025_06_20-15_36_53\n",
      "task_idx: 0\n",
      "episode_000011.parquet is generated: training_data/data/chunk-000/episode_000011.parquet\n",
      "****************************************************************************************************\n",
      "12: collect_data/2025_06_20-15_37_30\n",
      "task_idx: 0\n",
      "episode_000012.parquet is generated: training_data/data/chunk-000/episode_000012.parquet\n",
      "****************************************************************************************************\n",
      "13: collect_data/2025_06_20-15_38_15\n",
      "task_idx: 0\n",
      "episode_000013.parquet is generated: training_data/data/chunk-000/episode_000013.parquet\n",
      "****************************************************************************************************\n",
      "14: collect_data/2025_06_20-15_39_34\n",
      "task_idx: 0\n",
      "episode_000014.parquet is generated: training_data/data/chunk-000/episode_000014.parquet\n",
      "****************************************************************************************************\n",
      "15: collect_data/2025_06_20-15_40_26\n",
      "task_idx: 0\n",
      "episode_000015.parquet is generated: training_data/data/chunk-000/episode_000015.parquet\n",
      "****************************************************************************************************\n",
      "16: collect_data/2025_06_20-15_41_11\n",
      "task_idx: 0\n",
      "episode_000016.parquet is generated: training_data/data/chunk-000/episode_000016.parquet\n",
      "****************************************************************************************************\n",
      "17: collect_data/2025_06_20-15_42_07\n",
      "task_idx: 0\n",
      "episode_000017.parquet is generated: training_data/data/chunk-000/episode_000017.parquet\n",
      "****************************************************************************************************\n",
      "18: collect_data/2025_06_20-15_42_59\n",
      "task_idx: 0\n",
      "episode_000018.parquet is generated: training_data/data/chunk-000/episode_000018.parquet\n",
      "****************************************************************************************************\n",
      "19: collect_data/2025_06_20-15_43_38\n",
      "task_idx: 0\n",
      "episode_000019.parquet is generated: training_data/data/chunk-000/episode_000019.parquet\n",
      "****************************************************************************************************\n",
      "20: collect_data/2025_06_20-15_44_26\n",
      "task_idx: 0\n",
      "episode_000020.parquet is generated: training_data/data/chunk-000/episode_000020.parquet\n",
      "****************************************************************************************************\n",
      "21: collect_data/2025_06_20-15_45_02\n",
      "task_idx: 0\n",
      "episode_000021.parquet is generated: training_data/data/chunk-000/episode_000021.parquet\n",
      "****************************************************************************************************\n",
      "22: collect_data/2025_06_20-15_46_17\n",
      "task_idx: 0\n",
      "episode_000022.parquet is generated: training_data/data/chunk-000/episode_000022.parquet\n",
      "****************************************************************************************************\n",
      "23: collect_data/2025_06_20-15_47_07\n",
      "task_idx: 0\n",
      "episode_000023.parquet is generated: training_data/data/chunk-000/episode_000023.parquet\n",
      "****************************************************************************************************\n",
      "24: collect_data/2025_06_20-15_47_48\n",
      "task_idx: 0\n",
      "episode_000024.parquet is generated: training_data/data/chunk-000/episode_000024.parquet\n",
      "****************************************************************************************************\n",
      "25: collect_data/2025_06_20-15_48_33\n",
      "task_idx: 0\n",
      "episode_000025.parquet is generated: training_data/data/chunk-000/episode_000025.parquet\n",
      "****************************************************************************************************\n",
      "26: collect_data/2025_06_20-15_49_27\n",
      "task_idx: 0\n",
      "episode_000026.parquet is generated: training_data/data/chunk-000/episode_000026.parquet\n",
      "****************************************************************************************************\n",
      "27: collect_data/2025_06_20-15_50_05\n",
      "task_idx: 0\n",
      "episode_000027.parquet is generated: training_data/data/chunk-000/episode_000027.parquet\n",
      "****************************************************************************************************\n",
      "28: collect_data/2025_06_20-15_50_45\n",
      "task_idx: 0\n",
      "episode_000028.parquet is generated: training_data/data/chunk-000/episode_000028.parquet\n",
      "****************************************************************************************************\n",
      "29: collect_data/2025_06_20-15_51_22\n",
      "task_idx: 0\n",
      "episode_000029.parquet is generated: training_data/data/chunk-000/episode_000029.parquet\n"
     ]
    }
   ],
   "source": [
    "global_idx = 0\n",
    "\n",
    "for idx, folder in enumerate(sorted(SOURCE_DIR.iterdir())):    \n",
    "    print(\"*\" * 100)\n",
    "    print(f\"{idx}: {folder}\")\n",
    "    \n",
    "\n",
    "    global_idx = parquet_file_generation(idx,folder, global_idx)\n",
    "    \n",
    "    # video_generation(idx,folder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df8419f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12042\n"
     ]
    }
   ],
   "source": [
    "lengths = [\n",
    "    475, 379, 387, 328, 304, 334, 418, 358, 318, 534,\n",
    "    346, 320, 389, 372, 451, 441, 380, 406, 446, 445,\n",
    "    385, 489, 510, 447, 447, 484, 314, 418, 331, 386\n",
    "]\n",
    "\n",
    "total_length = sum(lengths)\n",
    "print(total_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr00t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
